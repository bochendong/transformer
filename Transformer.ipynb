{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 27"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # [64, 8, 5, 5]\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, dropout, batch_size = batch_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // nhead\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        assert self.head_dim * nhead == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
    "        nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "\n",
    "        self.out_proj_weight =  Parameter(torch.empty((embed_dim, embed_dim)))\n",
    "        nn.init.xavier_uniform_(self.out_proj_weight)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask = None):\n",
    "        seq_length, batch_size, embed_dim = query.size()                                            # query size (5, 64, 256)\n",
    "\n",
    "        qkv = torch._C._nn.linear(query, self.in_proj_weight)\n",
    "\n",
    "        qkv = qkv.unflatten(-1, (3, self.embed_dim)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()  # qkv (5, 64, 768)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]                                                            # (5, 64, 256)\n",
    "\n",
    "        q = q.view(seq_length, batch_size * self.nhead, self.head_dim).transpose(0, 1)              # [512, 5, 32]\n",
    "        k = k.view(seq_length, batch_size * self.nhead, self.head_dim).transpose(0, 1)\n",
    "        v = v.view(seq_length, batch_size * self.nhead, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        q = q.view(batch_size, self.nhead, seq_length, self.head_dim)                               # [64, 8, 5, 32]\n",
    "        k = k.view(batch_size, self.nhead, seq_length, self.head_dim)\n",
    "        v = v.view(batch_size, self.nhead, seq_length, self.head_dim)\n",
    "\n",
    "        values, _ = scaled_dot_product(q, k, v)                                                     # [64, 8, 5, 32]\n",
    "        values = values.permute(2, 0, 1, 3).contiguous().view(batch_size * seq_length, embed_dim)   # [320, 256]\n",
    "\n",
    "        o = torch._C._nn.linear(values, self.out_proj_weight)\n",
    "        o = o.view(seq_length, batch_size, embed_dim)\n",
    "\n",
    "        return o\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.attention = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "    def forward(self, x, src_mask = None, src_key_padding_mask = None, is_causal = False):\n",
    "        # sa\n",
    "        sa = self.dropout1(self.attention(x, x, x, attn_mask = src_mask))\n",
    "        x = self.norm1(x + sa)\n",
    "\n",
    "        # ff\n",
    "        ff = self.dropout2(self.linear2(self.dropout(self.activation(self.linear1(x)))))\n",
    "        x = self.norm2(x + ff)\n",
    "\n",
    "        return x\n",
    "\n",
    "import copy\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    # FIXME: copy.deepcopy() is not defined on nn.module\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None): #, ** block_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        # self.layers = nn.ModuleList([EncoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(x, src_mask)\n",
    "\n",
    "        # output = torch.nested.to_padded_tensor(output, 0.)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, norm=None, ** block_args):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = nn.ModuleList([EncoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(x, src_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask, memory_mask):\n",
    "        sa = self.self_attn(x, x, x, attn_mask=tgt_mask)[0]\n",
    "        x = self.norm1(x + self.dropout1(sa))\n",
    "\n",
    "        ma = self.multihead_attn(x, memory, memory, attn_mask=memory_mask)[0]\n",
    "        x = self.norm2(x + self.dropout2(ma))\n",
    "\n",
    "        ff = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = self.norm3(x + self.dropout2(ff))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, norm = None, ** block_args):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask = None, memory_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(tgt, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model = 100, nhead = 2, num_encoder_layers = 3, num_decoder_layers = 3, dim_feedforward = 64, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        block_args = {'d_model': d_model, 'nhead': nhead, 'dim_feedforward': dim_feedforward, 'dropout': dropout}\n",
    "\n",
    "        encoder_norm = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.encoder = TransformerEncoder(num_encoder_layers, encoder_norm, **block_args)\n",
    "\n",
    "        decoder_norm =nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.decoder = TransformerDecoder(num_decoder_layers, decoder_norm, **block_args)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, memory_mask = None):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
