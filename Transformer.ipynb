{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 27"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, nhead, dropout = 0.1, batch_size = batch_size, seq_length = seq_length):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // nhead\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        assert self.head_dim * nhead == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim)))\n",
    "\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask = None):\n",
    "        batch_size, seq_length, embed_dim = query.shape\n",
    "        print(\"batch_size\", batch_size, 'seq_length', seq_length, 'embed_dim',  embed_dim)\n",
    "        if attn_mask == None:\n",
    "            if query is key and key is value:\n",
    "                qkv = torch._C._nn.linear(query, self.in_proj_weight)\n",
    "                qkv = qkv.unflatten(-1, (3, self.embed_dim)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
    "                q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "            else:\n",
    "                w_q, w_k, w_v = self.in_proj_weight.chunk(3)\n",
    "                q, k, v = torch._C._nn.linear(query, w_q), torch._C._nn.linear(key, w_k), torch._C._nn.linear(value, w_v)\n",
    "            \n",
    "            print('q', q.size())\n",
    "            print('k', k.size())\n",
    "            print('v', v.size())\n",
    "            # batch_size, seq_length, self.nhead, 3*self.head_dim\n",
    "            q = q.view(batch_size, seq_length, self.nhead, self.head_dim)\n",
    "            k = k.view(batch_size, seq_length, self.nhead, self.head_dim)\n",
    "            v = v.view(batch_size, seq_length, self.nhead, self.head_dim)\n",
    "\n",
    "            q = q.permute(0, 2, 1, 3)\n",
    "            k = k.permute(0, 2, 1, 3)\n",
    "            v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "            print(q.size())\n",
    "           \n",
    "            values, attention = scaled_dot_product(q, k, v)\n",
    "            values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "            values = values.reshape(batch_size, seq_length, self.q)\n",
    "\n",
    "            o = self.o_proj(values)\n",
    "            return o, attention\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.attention = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        # sa\n",
    "        attn = self.attention(x, x, x, attn_mask = src_mask)[0]\n",
    "        x = x + self.dropout1(attn)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # ff\n",
    "        x = x + self.dropout2(self.linear2(self.dropout(self.activation(self.linear1(x)))))\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, norm=None, ** block_args):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(x, src_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout = dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask, memory_mask):\n",
    "        sa = self.self_attn(x, x, x, attn_mask=tgt_mask)[0]\n",
    "        x = self.norm1(x + self.dropout1(sa))\n",
    "\n",
    "        ma = self.multihead_attn(x, memory, memory, attn_mask=memory_mask)[0]\n",
    "        x = self.norm2(x + self.dropout2(ma))\n",
    "\n",
    "        ff = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        x = self.norm3(x + self.dropout2(ff))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, norm = None, ** block_args):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask = None, memory_mask = None):\n",
    "        for mod in self.layers:\n",
    "            output = mod(tgt, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model = 100, nhead = 2, num_encoder_layers = 3, num_decoder_layers = 3, dim_feedforward = 64, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        block_args = {'d_model': d_model, 'nhead': nhead, 'dim_feedforward': dim_feedforward, 'dropout': dropout}\n",
    "\n",
    "        encoder_norm = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.encoder = TransformerEncoder(num_encoder_layers, encoder_norm, **block_args)\n",
    "\n",
    "        decoder_norm =nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.decoder = TransformerDecoder(num_decoder_layers, decoder_norm, **block_args)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, memory_mask = None):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
